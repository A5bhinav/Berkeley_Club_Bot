{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7827b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the stored dataframes from the previous notebook using the pickle module\n",
    "import pandas as pd\n",
    "consulting_club_posts = pd.read_pickle('consulting_club_posts.pkl')\n",
    "comments_df = pd.read_pickle('comments_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4373e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "# embed_model = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'    Can't take large enough data\n",
    "#Use the command olllama serve in the terminal to start the ollama server\n",
    "embed_model = 'hf.co/bartowski/granite-embedding-30m-english-GGUF'\n",
    "lang_model = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "ollama.pull(embed_model)\n",
    "ollama.pull(lang_model)\n",
    "\n",
    "consulting_club_posts['embedding'] = consulting_club_posts['Post Text'].apply(\n",
    "    lambda t: ollama.embed(model=embed_model, input=t)['embeddings'])\n",
    "\n",
    "comments_df['embedding'] = comments_df['Comment'].apply(\n",
    "    lambda t: ollama.embed(model=embed_model, input=t)['embeddings'])\n",
    "\n",
    "# Formats it so each element is not a list of a list unnecessarily\n",
    "consulting_club_posts['embedding'] = consulting_club_posts['embedding'].apply(lambda x: x[0] if len(x) == 1 else x)\n",
    "comments_df['embedding'] = comments_df['embedding'].apply(lambda x: x[0] if len(x) == 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7964e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating weights for each embedding based on score values, plugging them into a tuned sigmoid function\n",
    "# 0-200 has factor ~1, 200-500 has factor ~1.3, 500-700 has factor ~1.6, 700+ has factor ~1.8\n",
    "# Doesn't function well, need to include comment score scaling as well if implemented\n",
    "\n",
    "# consulting_club_posts['Score Factor'] = consulting_club_posts['Score'].apply(lambda x: 1 + 1/(1+20*pow(2, -x/100)))\n",
    "\n",
    "# consulting_club_posts['Weighted embedding'] = consulting_club_posts['embedding'].combine(consulting_club_posts['Score Factor'], lambda lst, weight: [weight * x for x in lst])\n",
    "# print(consulting_club_posts['Weighted embedding'], consulting_club_posts['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eca202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dot_prod = sum([x * y for x, y in zip(a, b)])\n",
    "    mag_a = pow(sum([pow(x, 2) for x in a]), 0.5)\n",
    "    mag_b = pow(sum([pow(y, 2) for y in b]), 0.5)\n",
    "    return mag_a * mag_b and (dot_prod) / (mag_a * mag_b)   # add mag_b to dot_prod for weightage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcca088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(query, n=5):\n",
    "    query_embed = ollama.embed(model=embed_model, input=query)['embeddings']\n",
    "    similarities = consulting_club_posts['embedding'].apply(lambda x: cosine_similarity(query_embed[0], x))     # Change to 'weighted embedding' for weightage\n",
    "    pd.concat([similarities, comments_df['embedding'].apply(lambda x: cosine_similarity(query_embed[0], x))])   \n",
    "    return similarities.nlargest(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "retrieved_knowledge = retrieve_data(input_query)\n",
    "\n",
    "\n",
    "instruction_prompt = f'''You are a helpful chatbot aimed to help UC Berkeley students learn about and choose clubs to join.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{'\\n'.join([f' - {consulting_club_posts.loc[i, 'Post Text']}' for i in retrieved_knowledge.index])}\n",
    "'''\n",
    "\n",
    "print(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "  model=lang_model,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "# print the response from the chatbot in real-time\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
