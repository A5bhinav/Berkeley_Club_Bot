{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from html import unescape\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Regular requests for the EECS page (no JS needed there)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium for organization websites (handles JS)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from praw.models import MoreComments #This import is needed is used only if we want to limit the comments we scrape\n",
    "\n",
    "# #URL to access the app needed to scrape the data off the Berkeley subreddit\n",
    "# # https://www.reddit.com/prefs/apps\n",
    "\n",
    "#This is the tutorial I used to set up the web scraping using PRAW\n",
    "#https://www.geeksforgeeks.org/python/scraping-reddit-using-python/\n",
    "\n",
    "# #PLAN\n",
    "# #Scrape the data off the Berkeley subreddit about consulting clubs\n",
    "# #Organize that data using the pandas library\n",
    "# #Create chatbot that utilizes natural language processing that will give users feedback\n",
    "# #about each consulting club here at Berkeley. \n",
    "\n",
    "reddit_read_only = praw.Reddit(client_id = \"QlBfNfxQ3e_MGP9RkaOQig\",\n",
    "                               client_secret = \"SpLjOwYdQPU4z1wqcXBjVl_7DnUIZg\",\n",
    "                               user_agent = \"Berkeley_Consulting\")\n",
    "\n",
    "subreddit = reddit_read_only.subreddit(\"berkeley\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178c2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulting_posts = subreddit.search('clubs', limit=None)\n",
    "\n",
    "posts_dict = {'Title': [], 'Post Text': [], 'ID': [], 'Score': [], 'Total Comments': [], 'Post URL': []}\n",
    "comments_dict = {'Comment': [], 'Score': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ecf18af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Post Text</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total Comments</th>\n",
       "      <th>Post URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Looking for people to start a bridge troll clu...</td>\n",
       "      <td>Hello, \\n\\nI was wondering if anyone else woul...</td>\n",
       "      <td>1nfcxn2</td>\n",
       "      <td>265</td>\n",
       "      <td>36</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/1nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heartbreaking club experience as a freshman</td>\n",
       "      <td>I applied to 9 tech clubs (i‚Äôm an eecs major) ...</td>\n",
       "      <td>1nh1egm</td>\n",
       "      <td>131</td>\n",
       "      <td>52</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/1nh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berkeley club decisions today got me like:</td>\n",
       "      <td>[Berkeley club decisions today got me like:](h...</td>\n",
       "      <td>1na5tea</td>\n",
       "      <td>71</td>\n",
       "      <td>50</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/1na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Potential nepotism in prestigious clubs is sic...</td>\n",
       "      <td>Title. So exhausted of this kinda stuff. Heard...</td>\n",
       "      <td>1nbpwed</td>\n",
       "      <td>108</td>\n",
       "      <td>36</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/1nb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asian monoculture in consulting clubs</td>\n",
       "      <td>Over the past month on this subreddit there ha...</td>\n",
       "      <td>16o5u8z</td>\n",
       "      <td>268</td>\n",
       "      <td>112</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/16o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>So you got rejected from a club. What now?</td>\n",
       "      <td>So you got rejected by a club at Berkeley. Pro...</td>\n",
       "      <td>sh9utu</td>\n",
       "      <td>161</td>\n",
       "      <td>24</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/sh9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Are there any clubs on campus for students who...</td>\n",
       "      <td>Basically what the title says! Or if there are...</td>\n",
       "      <td>1h0w654</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/1h0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>What are some fun clubs on campus?</td>\n",
       "      <td>Anyone have any recommendations?</td>\n",
       "      <td>198ng9e</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>How to get into Consulting clubs?</td>\n",
       "      <td>Hello! I'm going to be a sophomore next year a...</td>\n",
       "      <td>1ek2g0a</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/1ek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>[Club Recruitment Megathread] Have a club that...</td>\n",
       "      <td>Here is the format you should follow to ensure...</td>\n",
       "      <td>i9yljd</td>\n",
       "      <td>82</td>\n",
       "      <td>46</td>\n",
       "      <td>https://www.reddit.com/r/berkeley/comments/i9y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    Looking for people to start a bridge troll clu...   \n",
       "1          Heartbreaking club experience as a freshman   \n",
       "2           Berkeley club decisions today got me like:   \n",
       "3    Potential nepotism in prestigious clubs is sic...   \n",
       "4                Asian monoculture in consulting clubs   \n",
       "..                                                 ...   \n",
       "204         So you got rejected from a club. What now?   \n",
       "205  Are there any clubs on campus for students who...   \n",
       "206                 What are some fun clubs on campus?   \n",
       "207                  How to get into Consulting clubs?   \n",
       "208  [Club Recruitment Megathread] Have a club that...   \n",
       "\n",
       "                                             Post Text       ID  Score  \\\n",
       "0    Hello, \\n\\nI was wondering if anyone else woul...  1nfcxn2    265   \n",
       "1    I applied to 9 tech clubs (i‚Äôm an eecs major) ...  1nh1egm    131   \n",
       "2    [Berkeley club decisions today got me like:](h...  1na5tea     71   \n",
       "3    Title. So exhausted of this kinda stuff. Heard...  1nbpwed    108   \n",
       "4    Over the past month on this subreddit there ha...  16o5u8z    268   \n",
       "..                                                 ...      ...    ...   \n",
       "204  So you got rejected by a club at Berkeley. Pro...   sh9utu    161   \n",
       "205  Basically what the title says! Or if there are...  1h0w654      3   \n",
       "206                   Anyone have any recommendations?  198ng9e     14   \n",
       "207  Hello! I'm going to be a sophomore next year a...  1ek2g0a      4   \n",
       "208  Here is the format you should follow to ensure...   i9yljd     82   \n",
       "\n",
       "     Total Comments                                           Post URL  \n",
       "0                36  https://www.reddit.com/r/berkeley/comments/1nf...  \n",
       "1                52  https://www.reddit.com/r/berkeley/comments/1nh...  \n",
       "2                50  https://www.reddit.com/r/berkeley/comments/1na...  \n",
       "3                36  https://www.reddit.com/r/berkeley/comments/1nb...  \n",
       "4               112  https://www.reddit.com/r/berkeley/comments/16o...  \n",
       "..              ...                                                ...  \n",
       "204              24  https://www.reddit.com/r/berkeley/comments/sh9...  \n",
       "205               0  https://www.reddit.com/r/berkeley/comments/1h0...  \n",
       "206              16  https://www.reddit.com/r/berkeley/comments/198...  \n",
       "207               6  https://www.reddit.com/r/berkeley/comments/1ek...  \n",
       "208              46  https://www.reddit.com/r/berkeley/comments/i9y...  \n",
       "\n",
       "[209 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for post in consulting_posts:\n",
    "    if len(post.selftext) > 20:\n",
    "        #The title of the post\n",
    "        posts_dict['Title'].append(post.title)\n",
    "        #The text inside of the post\n",
    "        posts_dict['Post Text'].append(post.selftext)\n",
    "        #Unique ID of each post\n",
    "        posts_dict['ID'].append(post.id)\n",
    "        #The scoure of a post\n",
    "        posts_dict['Score'].append(post.score)\n",
    "        #Total number of comments inside the post\n",
    "        posts_dict['Total Comments'].append(post.num_comments)\n",
    "        #URL of each post\n",
    "        posts_dict['Post URL'].append(post.url)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "consulting_club_posts = pd.DataFrame(posts_dict)\n",
    "# print(consulting_club_posts)\n",
    "consulting_club_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01d1eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don‚Äôt forget riddles</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if you‚Äôre deadass, i will absolutely join</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm actually serious about starting it, if som...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>definitely don‚Äôt go barefoot under a bridge, t...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do we get to eat billy goats?</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>Hi! Student here looking for a club!\\n\\n1. I w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>**JOIN THE BERKELEY FORUM**\\n\\n1. **All Majors...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Hi all, we're Berkeley Phi Beta Lambda, a chap...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Hi Everyone! I'm representing Womxn in Math at...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Hi, posting for **Berkeley Psychology Group**!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Comment  Score\n",
       "0                                  Don‚Äôt forget riddles     67\n",
       "1             if you‚Äôre deadass, i will absolutely join     51\n",
       "2     I'm actually serious about starting it, if som...     49\n",
       "3     definitely don‚Äôt go barefoot under a bridge, t...     16\n",
       "4                         Do we get to eat billy goats?     12\n",
       "...                                                 ...    ...\n",
       "1043  Hi! Student here looking for a club!\\n\\n1. I w...      2\n",
       "1044  **JOIN THE BERKELEY FORUM**\\n\\n1. **All Majors...      2\n",
       "1045  Hi all, we're Berkeley Phi Beta Lambda, a chap...      2\n",
       "1046  Hi Everyone! I'm representing Womxn in Math at...      2\n",
       "1047  Hi, posting for **Berkeley Psychology Group**!...      1\n",
       "\n",
       "[1048 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to figure out a way to get the URL of each and every post\n",
    "for i in posts_dict['Post URL']:\n",
    "    if 'comments' in i:\n",
    "        submission = reddit_read_only.submission(url = i)\n",
    "    else:\n",
    "        continue\n",
    "    for comment in submission.comments:\n",
    "        if len(comment.body) < 20:\n",
    "            continue\n",
    "        comments_dict['Comment'].append(comment.body)\n",
    "        comments_dict['Score'].append(comment.score)\n",
    "\n",
    "# print(post_comments[0]) #This only prints one comment right now for one URL. Need to get as many comments as possible for one given URL.\n",
    "\n",
    "comments_df = pd.DataFrame(comments_dict)\n",
    "comments_df\n",
    "\n",
    "#Generate the CSV file that contains all the posts about Berkeley clubs\n",
    "# comments_df.to_csv('my_dataframe.csv', index=False) # index=False prevents writing the DataFrame index as a column\n",
    "\n",
    "#They are not marked in any way so they are just random pieces of information\n",
    "#Need to find a way to associate either the post title or the post URL with the post comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eba06ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Text</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, \\n\\nI was wondering if anyone else woul...</td>\n",
       "      <td>Don‚Äôt forget riddles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I applied to 9 tech clubs (i‚Äôm an eecs major) ...</td>\n",
       "      <td>if you‚Äôre deadass, i will absolutely join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Berkeley club decisions today got me like:](h...</td>\n",
       "      <td>I'm actually serious about starting it, if som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title. So exhausted of this kinda stuff. Heard...</td>\n",
       "      <td>definitely don‚Äôt go barefoot under a bridge, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over the past month on this subreddit there ha...</td>\n",
       "      <td>Do we get to eat billy goats?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi! Student here looking for a club!\\n\\n1. I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>NaN</td>\n",
       "      <td>**JOIN THE BERKELEY FORUM**\\n\\n1. **All Majors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi all, we're Berkeley Phi Beta Lambda, a chap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi Everyone! I'm representing Womxn in Math at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi, posting for **Berkeley Psychology Group**!...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Post Text  \\\n",
       "0     Hello, \\n\\nI was wondering if anyone else woul...   \n",
       "1     I applied to 9 tech clubs (i‚Äôm an eecs major) ...   \n",
       "2     [Berkeley club decisions today got me like:](h...   \n",
       "3     Title. So exhausted of this kinda stuff. Heard...   \n",
       "4     Over the past month on this subreddit there ha...   \n",
       "...                                                 ...   \n",
       "1043                                                NaN   \n",
       "1044                                                NaN   \n",
       "1045                                                NaN   \n",
       "1046                                                NaN   \n",
       "1047                                                NaN   \n",
       "\n",
       "                                                Comment  \n",
       "0                                  Don‚Äôt forget riddles  \n",
       "1             if you‚Äôre deadass, i will absolutely join  \n",
       "2     I'm actually serious about starting it, if som...  \n",
       "3     definitely don‚Äôt go barefoot under a bridge, t...  \n",
       "4                         Do we get to eat billy goats?  \n",
       "...                                                 ...  \n",
       "1043  Hi! Student here looking for a club!\\n\\n1. I w...  \n",
       "1044  **JOIN THE BERKELEY FORUM**\\n\\n1. **All Majors...  \n",
       "1045  Hi all, we're Berkeley Phi Beta Lambda, a chap...  \n",
       "1046  Hi Everyone! I'm representing Womxn in Math at...  \n",
       "1047  Hi, posting for **Berkeley Psychology Group**!...  \n",
       "\n",
       "[1048 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.concat([consulting_club_posts['Post Text'], comments_df['Comment']], axis=1)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e743c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading the stored dataframes from the previous notebook using the pickle module\n",
    "# consulting_club_posts.to_pickle('consulting_club_posts.pkl')\n",
    "# comments_df.to_pickle('comments_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9c0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CalLink Deep Scraper\n",
    "Fetches the organization list AND visits each club's individual page\n",
    "to get comprehensive information\n",
    "\"\"\"\n",
    "\n",
    "def strip_html(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags and clean up text\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e7d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_organization_list() -> List[Dict]:\n",
    "    \"\"\"Fetch the list of all organizations from the API\"\"\"\n",
    "    base_url = \"https://callink.berkeley.edu/api/discovery/search/organizations\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json',\n",
    "        'Referer': 'https://callink.berkeley.edu/Organizations',\n",
    "    }\n",
    "    \n",
    "    all_organizations = []\n",
    "    skip = 0\n",
    "    top = 100\n",
    "    \n",
    "    print(\"üìã Fetching organization list from CalLink API...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'top': top,\n",
    "            'skip': skip,\n",
    "            'orderBy[0]': 'UpperName asc',\n",
    "            'query': '',\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"  Fetching organizations {skip + 1} to {skip + top}...\", end=\" \")\n",
    "            response = requests.get(base_url, params=params, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Error: Status code {response.status_code}\")\n",
    "                break\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                orgs = data.get('value', data.get('items', data.get('results', [])))\n",
    "            elif isinstance(data, list):\n",
    "                orgs = data\n",
    "            else:\n",
    "                print(\"‚ùå Unexpected response format\")\n",
    "                break\n",
    "            \n",
    "            if not orgs:\n",
    "                print(\"‚úì Done\")\n",
    "                break\n",
    "            \n",
    "            all_organizations.extend(orgs)\n",
    "            print(f\"‚úì Got {len(orgs)} (Total: {len(all_organizations)})\")\n",
    "            \n",
    "            if len(orgs) < top:\n",
    "                break\n",
    "            \n",
    "            skip += top\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n‚úì Found {len(all_organizations)} organizations total\\n\")\n",
    "    return all_organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98667579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_organization_page(org_url: str, org_name: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape detailed information from an individual organization page\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(org_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        details = {}\n",
    "        \n",
    "        # Try to extract various fields from the page\n",
    "        # These selectors are educated guesses - we'll need to adjust based on actual HTML\n",
    "        \n",
    "        # Description (usually in a main content area)\n",
    "        desc_selectors = [\n",
    "            'div.organization-description',\n",
    "            'div.description',\n",
    "            'div[class*=\"about\"]',\n",
    "            'div[class*=\"description\"]',\n",
    "            'section.description',\n",
    "        ]\n",
    "        for selector in desc_selectors:\n",
    "            elem = soup.select_one(selector)\n",
    "            if elem:\n",
    "                details['full_description'] = strip_html(elem.get_text())\n",
    "                break\n",
    "        \n",
    "        # Purpose/Mission\n",
    "        purpose_selectors = [\n",
    "            'div.purpose',\n",
    "            'div[class*=\"mission\"]',\n",
    "            'div[class*=\"purpose\"]',\n",
    "        ]\n",
    "        for selector in purpose_selectors:\n",
    "            elem = soup.select_one(selector)\n",
    "            if elem:\n",
    "                details['purpose'] = strip_html(elem.get_text())\n",
    "                break\n",
    "        \n",
    "        # Meeting info\n",
    "        meeting_selectors = [\n",
    "            'div.meeting-info',\n",
    "            'div[class*=\"meeting\"]',\n",
    "            'span[class*=\"meeting\"]',\n",
    "        ]\n",
    "        for selector in meeting_selectors:\n",
    "            elem = soup.select_one(selector)\n",
    "            if elem:\n",
    "                details['meeting_info'] = strip_html(elem.get_text())\n",
    "                break\n",
    "        \n",
    "        # Contact email(s)\n",
    "        emails = soup.find_all('a', href=re.compile(r'mailto:'))\n",
    "        if emails:\n",
    "            details['emails'] = ', '.join([e.get('href', '').replace('mailto:', '') for e in emails])\n",
    "        \n",
    "        # Social media links\n",
    "        social_links = {}\n",
    "        social_patterns = {\n",
    "            'facebook': r'facebook\\.com',\n",
    "            'instagram': r'instagram\\.com',\n",
    "            'twitter': r'twitter\\.com|x\\.com',\n",
    "            'linkedin': r'linkedin\\.com',\n",
    "            'discord': r'discord\\.(gg|com)',\n",
    "        }\n",
    "        \n",
    "        for platform, pattern in social_patterns.items():\n",
    "            link = soup.find('a', href=re.compile(pattern))\n",
    "            if link:\n",
    "                social_links[platform] = link.get('href', '')\n",
    "        \n",
    "        if social_links:\n",
    "            details['social_media'] = json.dumps(social_links)\n",
    "        \n",
    "        # Website\n",
    "        website = soup.find('a', href=re.compile(r'^https?://(?!callink\\.berkeley)'))\n",
    "        if website:\n",
    "            details['website'] = website.get('href', '')\n",
    "        \n",
    "        # Try to get all text content as fallback\n",
    "        if not details:\n",
    "            main_content = soup.find('main') or soup.find('body')\n",
    "            if main_content:\n",
    "                details['full_content'] = strip_html(main_content.get_text())\n",
    "        \n",
    "        return details if details else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ebe5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_organizations_detailed(org_list: List[Dict], max_orgs: Optional[int] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Visit each organization's page and collect detailed information\n",
    "    \n",
    "    Args:\n",
    "        org_list: List of organizations from the API\n",
    "        max_orgs: Optional limit for testing (scrape only first N orgs)\n",
    "    \"\"\"\n",
    "    print(\"üîç Scraping detailed information from each organization page...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if max_orgs:\n",
    "        print(f\"‚ö†Ô∏è  TEST MODE: Only scraping first {max_orgs} organizations\\n\")\n",
    "        org_list = org_list[:max_orgs]\n",
    "    \n",
    "    enriched_orgs = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for idx, org in enumerate(org_list, 1):\n",
    "        # Get basic info from API response\n",
    "        org_data = {\n",
    "            'name': strip_html(org.get('Name', org.get('name', ''))),\n",
    "            'short_description': strip_html(org.get('Description', org.get('description', ''))),\n",
    "            'status': strip_html(org.get('Status', '')),\n",
    "            'email': org.get('Email', org.get('email', '')),\n",
    "        }\n",
    "        \n",
    "        # Get categories\n",
    "        if 'CategoryNames' in org:\n",
    "            cats = org['CategoryNames']\n",
    "            org_data['categories'] = ', '.join([strip_html(str(c)) for c in cats]) if isinstance(cats, list) else strip_html(str(cats))\n",
    "        \n",
    "        # Build profile URL\n",
    "        website_key = org.get('WebsiteKey', '')\n",
    "        if website_key:\n",
    "            org_data['profile_url'] = f\"https://callink.berkeley.edu/organization/{website_key}\"\n",
    "        else:\n",
    "            org_data['profile_url'] = ''\n",
    "        \n",
    "        # Scrape detailed page\n",
    "        print(f\"[{idx}/{len(org_list)}] {org_data['name'][:50]}...\", end=\" \")\n",
    "        \n",
    "        if org_data['profile_url']:\n",
    "            details = scrape_organization_page(org_data['profile_url'], org_data['name'])\n",
    "            \n",
    "            if details:\n",
    "                org_data.update(details)\n",
    "                print(\"‚úì\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  (basic info only)\")\n",
    "                failed_count += 1\n",
    "        else:\n",
    "            print(\"‚ùå (no URL)\")\n",
    "            failed_count += 1\n",
    "        \n",
    "        enriched_orgs.append(org_data)\n",
    "        \n",
    "        # Be respectful to the server\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Progress update every 50 orgs\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"\\n  Progress: {idx}/{len(org_list)} completed ({failed_count} failed)\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úì Completed! Successfully scraped {len(enriched_orgs) - failed_count}/{len(enriched_orgs)} organizations\")\n",
    "    print(f\"  ({failed_count} organizations had limited/no detailed info)\\n\")\n",
    "    \n",
    "    return enriched_orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "248d9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(orgs: List[Dict], prefix: str = 'callink_complete'):\n",
    "    \"\"\"Save scraped data to CSV and JSON\"\"\"\n",
    "    if not orgs:\n",
    "        print(\"‚ùå No data to save\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(orgs)\n",
    "    \n",
    "    # Save CSV\n",
    "    csv_file = f'{prefix}.csv'\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"‚úì Saved CSV: {csv_file}\")\n",
    "    \n",
    "    # Save JSON\n",
    "    json_file = f'{prefix}.json'\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(orgs, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úì Saved JSON: {json_file}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DATA SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total organizations: {len(df)}\")\n",
    "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "    for col in df.columns:\n",
    "        non_null = df[col].notna().sum()\n",
    "        pct = (non_null / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {col}: {non_null}/{len(df)} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nSample organizations:\")\n",
    "    print(df[['name', 'categories']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e063b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üêª BERKELEY CALLINK DEEP SCRAPER\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"This will scrape BOTH the organization list AND each club's page\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Get organization list\n",
    "    org_list = fetch_organization_list()\n",
    "    \n",
    "    if not org_list:\n",
    "        print(\"‚ùå Failed to fetch organization list\")\n",
    "        return\n",
    "    \n",
    "    # Ask user if they want to test first\n",
    "    print(f\"Found {len(org_list)} organizations.\")\n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"  1. Test mode - scrape first 10 organizations (recommended for testing)\")\n",
    "    print(\"  2. Scrape ALL organizations (will take 10-20 minutes)\")\n",
    "    print()\n",
    "    \n",
    "    # For non-interactive mode, default to test\n",
    "    # In interactive mode, you can uncomment the input() line\n",
    "    # choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    #***************************************************\n",
    "    #CHANGE THIS TO 1 \n",
    "    choice = \"0\"  # Default to test mode\n",
    "    #***************************************************\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        max_orgs = 10\n",
    "        print(f\"\\nüß™ TEST MODE: Scraping first {max_orgs} organizations\\n\")\n",
    "    else:\n",
    "        max_orgs = None\n",
    "        print(f\"\\nüöÄ FULL MODE: Scraping all {len(org_list)} organizations\\n\")\n",
    "        print(\"‚è±Ô∏è  This will take approximately 10-20 minutes...\")\n",
    "        print(\"    (Visiting ~1000 pages with 0.5s delay between requests)\\n\")\n",
    "    \n",
    "    # Step 2: Scrape detailed info\n",
    "    enriched_orgs = scrape_all_organizations_detailed(org_list, max_orgs=max_orgs)\n",
    "    \n",
    "    # Step 3: Save data\n",
    "    print(\"\\nüíæ Saving data...\")\n",
    "    prefix = 'callink_test' if max_orgs else 'callink_complete'\n",
    "    save_data(enriched_orgs, prefix=prefix)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ SCRAPING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if max_orgs:\n",
    "        print(\"\\nüí° TIP: This was a test run. To scrape all organizations,\")\n",
    "        print(\"   change 'choice = \\\"1\\\"' to 'choice = \\\"2\\\"' in the script.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # main()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dcbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>club_name</th>\n",
       "      <th>acronym</th>\n",
       "      <th>website</th>\n",
       "      <th>emails</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EECS organizations</td>\n",
       "      <td>The Association of Women in EE &amp; CS</td>\n",
       "      <td>AWE</td>\n",
       "      <td>http://awe.berkeley.edu</td>\n",
       "      <td>aweberkeley@gmail.com;biasbusters-admin@lists....</td>\n",
       "      <td>Formerly AUWICSEE. aweberkeley@gmail.com AWE i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EECS organizations</td>\n",
       "      <td>Blockchain at Berkeley</td>\n",
       "      <td></td>\n",
       "      <td>https://blockchain.berkeley.edu/</td>\n",
       "      <td>admin@blockchain.berkeley.edu</td>\n",
       "      <td>admin@blockchain.berkeley.edu Blockchain at Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EECS organizations</td>\n",
       "      <td>Cloud at Cal</td>\n",
       "      <td></td>\n",
       "      <td>https://sites.google.com/berkeley.edu/cloudatcal</td>\n",
       "      <td>cloudatcal@gmail.com</td>\n",
       "      <td>cloudatcal@gmail.com Cloud at California is UC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EECS organizations</td>\n",
       "      <td>EECS Transfers at Berkeley</td>\n",
       "      <td></td>\n",
       "      <td>https://callink.berkeley.edu/organization/eecs...</td>\n",
       "      <td></td>\n",
       "      <td>EECS Transfers at Berkeley aims to build a com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EECS organizations</td>\n",
       "      <td>Eta Kappa Nu (HKN) EECS Honor Society</td>\n",
       "      <td></td>\n",
       "      <td>http://hkn.eecs.berkeley.edu/</td>\n",
       "      <td>hkn@hkn.eecs.berkeley.edu</td>\n",
       "      <td>hkn@hkn.eecs.berkeley.edu Eta Kappa Nu (HKN) i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category                              club_name acronym  \\\n",
       "0  EECS organizations    The Association of Women in EE & CS     AWE   \n",
       "1  EECS organizations                 Blockchain at Berkeley           \n",
       "2  EECS organizations                           Cloud at Cal           \n",
       "3  EECS organizations             EECS Transfers at Berkeley           \n",
       "4  EECS organizations  Eta Kappa Nu (HKN) EECS Honor Society           \n",
       "\n",
       "                                             website  \\\n",
       "0                            http://awe.berkeley.edu   \n",
       "1                   https://blockchain.berkeley.edu/   \n",
       "2   https://sites.google.com/berkeley.edu/cloudatcal   \n",
       "3  https://callink.berkeley.edu/organization/eecs...   \n",
       "4                      http://hkn.eecs.berkeley.edu/   \n",
       "\n",
       "                                              emails  \\\n",
       "0  aweberkeley@gmail.com;biasbusters-admin@lists....   \n",
       "1                      admin@blockchain.berkeley.edu   \n",
       "2                               cloudatcal@gmail.com   \n",
       "3                                                      \n",
       "4                          hkn@hkn.eecs.berkeley.edu   \n",
       "\n",
       "                                         description  \n",
       "0  Formerly AUWICSEE. aweberkeley@gmail.com AWE i...  \n",
       "1  admin@blockchain.berkeley.edu Blockchain at Be...  \n",
       "2  cloudatcal@gmail.com Cloud at California is UC...  \n",
       "3  EECS Transfers at Berkeley aims to build a com...  \n",
       "4  hkn@hkn.eecs.berkeley.edu Eta Kappa Nu (HKN) i...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = \"https://eecs.berkeley.edu/people/students-2/organizations/\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# --- 1. Download page ---\n",
    "resp = requests.get(URL, headers=headers)\n",
    "resp.raise_for_status()\n",
    "html = resp.text\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# --- 2. Helper: find sections we care about ---\n",
    "section_titles = [\n",
    "    \"EECS organizations\",\n",
    "    \"EE organizations\",\n",
    "    \"CS organizations\",\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for section_title in section_titles:\n",
    "    # find the <h2> (or similar) whose text matches the section title\n",
    "    header = soup.find(\n",
    "        lambda tag: tag.name in [\"h2\", \"h3\"]\n",
    "        and tag.get_text(strip=True) == section_title\n",
    "    )\n",
    "    if header is None:\n",
    "        continue\n",
    "\n",
    "    # Walk over siblings until the next h2 (i.e., next big section)\n",
    "    sibling = header.find_next_sibling()\n",
    "    while sibling and sibling.name != \"h2\":\n",
    "        # Each club is under an h3 (or sometimes h4)\n",
    "        if sibling.name in [\"h3\", \"h4\"]:\n",
    "            club_header = sibling\n",
    "\n",
    "            full_title = club_header.get_text(\" \", strip=True)\n",
    "            # Try to pull acronym in parentheses at end of title\n",
    "            m = re.search(r\"\\(([^)]+)\\)\\s*$\", full_title)\n",
    "            acronym = m.group(1) if m else \"\"\n",
    "\n",
    "            # Main club name = title with trailing \"(ABC)\" removed\n",
    "            club_name = full_title\n",
    "            if m:\n",
    "                club_name = full_title[:m.start()].strip(\", \").strip()\n",
    "\n",
    "            # Website = first link in the header, if any\n",
    "            link_tag = club_header.find(\"a\")\n",
    "            website = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"\"\n",
    "\n",
    "            # Collect description + emails from following siblings\n",
    "            desc_parts = []\n",
    "            emails = set()\n",
    "\n",
    "            desc_sib = club_header.find_next_sibling()\n",
    "            while desc_sib and desc_sib.name not in [\"h3\", \"h4\", \"h2\"]:\n",
    "                text = desc_sib.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    desc_parts.append(text)\n",
    "\n",
    "                    # find email-like strings in the text\n",
    "                    for email in re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text):\n",
    "                        emails.add(email)\n",
    "\n",
    "                desc_sib = desc_sib.find_next_sibling()\n",
    "\n",
    "            description = \" \".join(desc_parts)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"category\": section_title,\n",
    "                    \"club_name\": club_name,\n",
    "                    \"acronym\": acronym,\n",
    "                    \"website\": website,\n",
    "                    \"emails\": \";\".join(sorted(emails)),\n",
    "                    \"description\": description,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        sibling = sibling.find_next_sibling()\n",
    "\n",
    "# --- 3. Save to CSV ---\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"data/berkeley_eecs_orgs.csv\", index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9be0334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('posts_and_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üêª BERKELEY EECS ORGANIZATIONS SCRAPER (SELENIUM)\n",
      "======================================================================\n",
      "Handles JavaScript-heavy sites properly\n",
      "======================================================================\n",
      "\n",
      "üìã Fetching EECS organizations page...\n",
      "======================================================================\n",
      "   Requesting: https://eecs.berkeley.edu/people/students-2/organizations/\n",
      "   Status code: 200\n",
      "   Found 53 h3 tags\n",
      "‚úì Found 42 organizations\n",
      "\n",
      "Found 42 organizations.\n",
      "\n",
      "‚öôÔ∏è  Running in TEST MODE (first 5 organizations)\n",
      "    (Change max_orgs=5 to max_orgs=None for full scrape)\n",
      "\n",
      "üß™ Scraping first 5 organizations\n",
      "\n",
      "üåê Starting Chrome browser...\n",
      "üîç Scraping organization websites with Selenium...\n",
      "======================================================================\n",
      "‚ö†Ô∏è  TEST MODE: Only scraping first 5 organizations\n",
      "\n",
      "[1/5] The Association of Women in EE & CS (AWE)... ‚úì (About page)\n",
      "[2/5] Bias Busters... ‚úì (About page)\n",
      "[3/5] Black Engineering and Science Student Association ... ‚úì (About page)\n",
      "[4/5] Blockchain at Berkeley... ‚úì (About page)\n",
      "[5/5] Cloud at Cal... ‚úì (Homepage)\n",
      "\n",
      "‚úì Scraping complete!\n",
      "  Success: 5/5\n",
      "    - About pages: 4\n",
      "    - Homepages: 1\n",
      "  Minimal content: 0/5\n",
      "  Failed: 0/5\n",
      "\n",
      "\n",
      "üíæ Saving data...\n",
      "‚úì Saved CSV: eecs_orgs_test.csv\n",
      "‚úì Saved JSON: eecs_orgs_test.json\n",
      "\n",
      "======================================================================\n",
      "üìä DATA SUMMARY\n",
      "======================================================================\n",
      "Total organizations: 5\n",
      "\n",
      "Columns (8):\n",
      "  ‚Ä¢ name: 5/5 (100.0%)\n",
      "  ‚Ä¢ website: 5/5 (100.0%)\n",
      "  ‚Ä¢ eecs_summary: 5 with content\n",
      "  ‚Ä¢ email: 5/5 (100.0%)\n",
      "  ‚Ä¢ scraped_url: 5/5 (100.0%)\n",
      "  ‚Ä¢ has_about_page: 5/5 (100.0%)\n",
      "  ‚Ä¢ full_content: 4 with substantial content (>100 chars)\n",
      "  ‚Ä¢ status: 5/5 (100.0%)\n",
      "\n",
      "Sample organizations:\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìå The Association of Women in EE & CS (AWE)\n",
      "   Website: http://awe.berkeley.edu\n",
      "   EECS Summary: Formerly AUWICSEE. aweberkeley@gmail.com AWE is a supportive community at UC Berkeley that seeks to empower women pursuing Computer Science and/or Ele...\n",
      "   Email: aweberkeley@gmail.com\n",
      "   Scraped URL: https://awe.studentorg.berkeley.edu/home\n",
      "   Status: success\n",
      "   Website Content: AWE is a student-run organization at UC Berkeley that seeks to empower female and non-binary undergraduate students pursuing Computer Science and/or E...\n",
      "\n",
      "üìå Bias Busters\n",
      "   Website: https://biasbusters.berkeley.edu/\n",
      "   EECS Summary: biasbusters-admin@lists.eecs.berkeley.edu Bias Busters is an organization run by students, faculty, and staff to address implicit bias issues in the E...\n",
      "   Email: biasbusters-admin@lists.eecs.berkeley.edu\n",
      "   Scraped URL: https://biasbusters.berkeley.edu/about/\n",
      "   Status: success\n",
      "   Website Content: Home > AboutAbout Bias Busters is an organization run by graduate students, faculty, and staff to address implicit bias issues in our community. Our g...\n",
      "\n",
      "üìå Black Engineering and Science Student Association (BESSA)\n",
      "   Website: http://ucberkeleybessa.com\n",
      "   EECS Summary: ucberkeleybessa@gmail.com\n",
      "   Email: biasbusters-admin@lists.eecs.berkeley.edu\n",
      "   Scraped URL: http://ucberkeleybessa.com/about-us.html\n",
      "   Status: success\n",
      "   Website Content: ‚ÄãThe Black Engineering and Science Student Association, also known as BESSA, is the University of California, Berkeley chapter for our parent organiza...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "‚úÖ SCRAPING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üí° TIP: To scrape all organizations, change:\n",
      "   max_orgs = 5\n",
      "   to\n",
      "   max_orgs = None\n",
      "\n",
      "üîí Closing browser...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "EECS Organizations Scraper with Selenium\n",
    "Handles JavaScript-heavy sites properly\n",
    "\"\"\"\n",
    "\n",
    "def strip_html(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags and clean up text\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7da269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    \"\"\"Setup Chrome driver with appropriate options\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in background\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(20)  # 20 second timeout\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_organizations_from_eecs_page() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract all organizations and their info from the EECS organizations page\n",
    "    \"\"\"\n",
    "    url = \"https://eecs.berkeley.edu/people/students-2/organizations/\"\n",
    "    \n",
    "    print(\"üìã Fetching EECS organizations page...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        print(f\"   Requesting: {url}\")\n",
    "        response = requests.get(url, timeout=15)\n",
    "        print(f\"   Status code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"   ‚ùå Bad status code: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        organizations = []\n",
    "        \n",
    "        # Find all organization entries (they're in h3 tags with links)\n",
    "        h3_tags = soup.find_all('h3')\n",
    "        print(f\"   Found {len(h3_tags)} h3 tags\")\n",
    "        \n",
    "        for h3 in h3_tags:\n",
    "            link = h3.find('a', href=True)\n",
    "            if link:\n",
    "                org = {\n",
    "                    'name': strip_html(h3.get_text()),\n",
    "                    'website': link['href'],\n",
    "                    'eecs_summary': '',\n",
    "                    'email': ''\n",
    "                }\n",
    "                \n",
    "                # Get the description/summary from the EECS page\n",
    "                next_elem = h3.find_next_sibling()\n",
    "                desc_parts = []\n",
    "                \n",
    "                while next_elem and next_elem.name != 'h3':\n",
    "                    if next_elem.name == 'p':\n",
    "                        text = strip_html(next_elem.get_text())\n",
    "                        if text and not text.startswith('mailto:'):\n",
    "                            desc_parts.append(text)\n",
    "                    next_elem = next_elem.find_next_sibling()\n",
    "                \n",
    "                org['eecs_summary'] = ' '.join(desc_parts)\n",
    "                \n",
    "                # Get email\n",
    "                parent = h3.parent\n",
    "                if parent:\n",
    "                    email_link = parent.find('a', href=re.compile(r'^mailto:'))\n",
    "                    if email_link:\n",
    "                        org['email'] = email_link['href'].replace('mailto:', '')\n",
    "                \n",
    "                if not org['email']:\n",
    "                    next_elem = h3.find_next_sibling()\n",
    "                    while next_elem and next_elem.name != 'h3':\n",
    "                        email_link = next_elem.find('a', href=re.compile(r'^mailto:'))\n",
    "                        if email_link:\n",
    "                            org['email'] = email_link['href'].replace('mailto:', '')\n",
    "                            break\n",
    "                        next_elem = next_elem.find_next_sibling()\n",
    "                \n",
    "                organizations.append(org)\n",
    "        \n",
    "        print(f\"‚úì Found {len(organizations)} organizations\\n\")\n",
    "        return organizations\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"‚ùå Timeout error - the EECS website is taking too long to respond\")\n",
    "        print(f\"   Try running again or check your internet connection\")\n",
    "        return []\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"‚ùå Connection error - cannot reach eecs.berkeley.edu\")\n",
    "        print(f\"   Error: {str(e)[:100]}\")\n",
    "        print(f\"   Make sure you have internet access\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error fetching EECS page: {type(e).__name__}\")\n",
    "        print(f\"   Details: {str(e)[:200]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b558778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_about_page_selenium(driver, base_url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try to find an About page link using Selenium\n",
    "    \"\"\"\n",
    "    about_patterns = [\n",
    "        r'about',\n",
    "        r'who-we-are',\n",
    "        r'our-team',\n",
    "        r'mission',\n",
    "        r'overview'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Get all links on the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in all_links:\n",
    "            href = link['href'].lower()\n",
    "            link_text = link.get_text().lower()\n",
    "            \n",
    "            for pattern in about_patterns:\n",
    "                if pattern in href or pattern in link_text:\n",
    "                    about_url = urljoin(base_url, link['href'])\n",
    "                    return about_url\n",
    "        \n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website_selenium(driver, url: str, org_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Use Selenium to scrape an organization's website\n",
    "    Handles JavaScript-heavy sites\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'scraped_url': '',\n",
    "        'has_about_page': False,\n",
    "        'full_content': '',\n",
    "        'status': 'failed'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the homepage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for page to load\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            time.sleep(3)  # Extra time for JS to execute\n",
    "        except TimeoutException:\n",
    "            result['status'] = 'timeout'\n",
    "            return result\n",
    "        \n",
    "        # Try to find an About page\n",
    "        about_url = find_about_page_selenium(driver, url)\n",
    "        \n",
    "        if about_url and about_url != url:\n",
    "            # Navigate to About page\n",
    "            try:\n",
    "                driver.get(about_url)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "                time.sleep(3)\n",
    "                result['scraped_url'] = about_url\n",
    "                result['has_about_page'] = True\n",
    "            except:\n",
    "                # About page failed, use homepage\n",
    "                driver.get(url)\n",
    "                time.sleep(3)\n",
    "                result['scraped_url'] = url\n",
    "                result['has_about_page'] = False\n",
    "        else:\n",
    "            result['scraped_url'] = url\n",
    "            result['has_about_page'] = False\n",
    "        \n",
    "        # Extract content using BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Try to find main content\n",
    "        main_content = (\n",
    "            soup.find('main') or \n",
    "            soup.find('article') or \n",
    "            soup.find('div', {'role': 'main'}) or\n",
    "            soup.find('div', class_=re.compile(r'content|main', re.I)) or\n",
    "            soup.find('body')\n",
    "        )\n",
    "        \n",
    "        if main_content:\n",
    "            text = strip_html(main_content.get_text())\n",
    "            \n",
    "            # Accept any content over 50 chars, no upper limit\n",
    "            if len(text) > 50:\n",
    "                result['full_content'] = text  # No limit - get everything!\n",
    "                result['status'] = 'success'\n",
    "            else:\n",
    "                result['full_content'] = text\n",
    "                result['status'] = 'minimal_content'\n",
    "        else:\n",
    "            result['status'] = 'no_content_found'\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except TimeoutException:\n",
    "        result['status'] = 'timeout'\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        result['status'] = f'error: {str(e)[:50]}'\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_organizations(driver, orgs: List[Dict], max_orgs: Optional[int] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape detailed information from each organization's website using Selenium\n",
    "    \"\"\"\n",
    "    print(\"üîç Scraping organization websites with Selenium...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if max_orgs:\n",
    "        print(f\"‚ö†Ô∏è  TEST MODE: Only scraping first {max_orgs} organizations\\n\")\n",
    "        orgs = orgs[:max_orgs]\n",
    "    \n",
    "    enriched_orgs = []\n",
    "    stats = {'success': 0, 'failed': 0, 'about_pages': 0, 'homepages': 0, 'minimal': 0}\n",
    "    \n",
    "    for idx, org in enumerate(orgs, 1):\n",
    "        print(f\"[{idx}/{len(orgs)}] {org['name'][:50]}...\", end=\" \")\n",
    "        \n",
    "        if not org['website']:\n",
    "            print(\"‚ùå (no website)\")\n",
    "            org['scraped_url'] = ''\n",
    "            org['has_about_page'] = False\n",
    "            org['full_content'] = ''\n",
    "            org['status'] = 'no_website'\n",
    "            stats['failed'] += 1\n",
    "        else:\n",
    "            # Scrape the website with Selenium\n",
    "            scrape_result = scrape_website_selenium(driver, org['website'], org['name'])\n",
    "            org.update(scrape_result)\n",
    "            \n",
    "            if scrape_result['status'] == 'success':\n",
    "                if scrape_result['has_about_page']:\n",
    "                    print(\"‚úì (About page)\")\n",
    "                    stats['about_pages'] += 1\n",
    "                else:\n",
    "                    print(\"‚úì (Homepage)\")\n",
    "                    stats['homepages'] += 1\n",
    "                stats['success'] += 1\n",
    "            elif scrape_result['status'] == 'minimal_content':\n",
    "                print(\"‚ö†Ô∏è  (minimal content - may be JS-heavy)\")\n",
    "                stats['minimal'] += 1\n",
    "            else:\n",
    "                print(f\"‚ùå ({scrape_result['status']})\")\n",
    "                stats['failed'] += 1\n",
    "        \n",
    "        enriched_orgs.append(org)\n",
    "        \n",
    "        # Small delay between requests\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Progress update\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"\\n  Progress: {stats['success']} success, {stats['minimal']} minimal, {stats['failed']} failed\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úì Scraping complete!\")\n",
    "    print(f\"  Success: {stats['success']}/{len(orgs)}\")\n",
    "    print(f\"    - About pages: {stats['about_pages']}\")\n",
    "    print(f\"    - Homepages: {stats['homepages']}\")\n",
    "    print(f\"  Minimal content: {stats['minimal']}/{len(orgs)}\")\n",
    "    print(f\"  Failed: {stats['failed']}/{len(orgs)}\\n\")\n",
    "    \n",
    "    return enriched_orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(orgs: List[Dict], prefix: str = 'eecs_organizations'):\n",
    "    \"\"\"Save scraped data to CSV and JSON\"\"\"\n",
    "    if not orgs:\n",
    "        print(\"‚ùå No data to save\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(orgs)\n",
    "    \n",
    "    # Save CSV\n",
    "    csv_file = f'{prefix}.csv'\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "    print(f\"‚úì Saved CSV: {csv_file}\")\n",
    "    \n",
    "    # Save JSON\n",
    "    json_file = f'{prefix}.json'\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(orgs, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úì Saved JSON: {json_file}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DATA SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total organizations: {len(df)}\")\n",
    "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "    for col in df.columns:\n",
    "        non_null = df[col].notna().sum()\n",
    "        if col == 'full_content':\n",
    "            non_empty = (df[col].str.len() > 100).sum() if 'full_content' in df else 0\n",
    "            print(f\"  ‚Ä¢ {col}: {non_empty} with substantial content (>100 chars)\")\n",
    "        elif col == 'eecs_summary':\n",
    "            non_empty = (df[col].str.len() > 10).sum() if 'eecs_summary' in df else 0\n",
    "            print(f\"  ‚Ä¢ {col}: {non_empty} with content\")\n",
    "        else:\n",
    "            pct = (non_null / len(df)) * 100\n",
    "            print(f\"  ‚Ä¢ {col}: {non_null}/{len(df)} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample organizations:\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    for idx, row in df.head(3).iterrows():\n",
    "        print(f\"\\nüìå {row['name']}\")\n",
    "        print(f\"   Website: {row['website']}\")\n",
    "        if row.get('eecs_summary'):\n",
    "            summary_preview = row['eecs_summary'][:150] + \"...\" if len(row['eecs_summary']) > 150 else row['eecs_summary']\n",
    "            print(f\"   EECS Summary: {summary_preview}\")\n",
    "        if row.get('email'):\n",
    "            print(f\"   Email: {row['email']}\")\n",
    "        print(f\"   Scraped URL: {row.get('scraped_url', 'N/A')}\")\n",
    "        print(f\"   Status: {row.get('status', 'N/A')}\")\n",
    "        if row.get('full_content'):\n",
    "            content_preview = row['full_content'][:150] + \"...\" if len(row['full_content']) > 150 else row['full_content']\n",
    "            print(f\"   Website Content: {content_preview}\")\n",
    "    print(\"\\n\" + \"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "450ea459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üêª BERKELEY EECS ORGANIZATIONS SCRAPER (SELENIUM)\n",
      "======================================================================\n",
      "Handles JavaScript-heavy sites properly\n",
      "======================================================================\n",
      "\n",
      "üìã Fetching EECS organizations page...\n",
      "======================================================================\n",
      "   Requesting: https://eecs.berkeley.edu/people/students-2/organizations/\n",
      "   Status code: 200\n",
      "   Found 53 h3 tags\n",
      "‚úì Found 42 organizations\n",
      "\n",
      "Found 42 organizations.\n",
      "\n",
      "‚öôÔ∏è  Running in TEST MODE (first 5 organizations)\n",
      "    (Change max_orgs=5 to max_orgs=None for full scrape)\n",
      "\n",
      "üöÄ Scraping all 42 organizations\n",
      "‚è±Ô∏è  Estimated time: 3-4 minutes\n",
      "\n",
      "üåê Starting Chrome browser...\n",
      "üîç Scraping organization websites with Selenium...\n",
      "======================================================================\n",
      "[1/42] The Association of Women in EE & CS (AWE)... ‚úì (About page)\n",
      "[2/42] Bias Busters... ‚úì (About page)\n",
      "[3/42] Black Engineering and Science Student Association ... ‚úì (About page)\n",
      "[4/42] Blockchain at Berkeley... ‚úì (About page)\n",
      "[5/42] Cloud at Cal... ‚úì (Homepage)\n",
      "[6/42] EECS Transfers at Berkeley... ‚úì (Homepage)\n",
      "[7/42] Eta Kappa Nu (HKN) EECS Honor Society... ‚úì (Homepage)\n",
      "[8/42] Graduates for Engaged and Extended Scholarship aro... ‚úì (About page)\n",
      "[9/42] Machine Learning at Berkeley (ML@B)... ‚úì (About page)\n",
      "[10/42] Neurotech@Berkeley... ‚ö†Ô∏è  (minimal content - may be JS-heavy)\n",
      "\n",
      "  Progress: 9 success, 1 minimal, 0 failed\n",
      "\n",
      "[11/42] Queer Graduate Students in Computer Science and El... ‚ö†Ô∏è  (minimal content - may be JS-heavy)\n",
      "[12/42] Robotics@Berkeley (R@B)... ‚úì (Homepage)\n",
      "[13/42] Women in Computer Science and Electrical Engineeri... ‚úì (About page)\n",
      "[14/42] Black Graduate Engineering and Science Students (B... ‚úì (Homepage)\n",
      "[15/42] Electrical Engineering Graduate Student Associatio... ‚úì (Homepage)\n",
      "[16/42] Formula Electric at Berkeley (EV)... ‚úì (Homepage)\n",
      "[17/42] Institute of Electrical and Electronics Engineers ... ‚úì (About page)\n",
      "[18/42] Society of Women Engineers (SWE)... ‚úì (About page)\n",
      "[19/42] ANova... ‚úì (About page)\n",
      "[20/42] BERKE1337 Computer Security Team... ‚úì (Homepage)\n",
      "\n",
      "  Progress: 18 success, 2 minimal, 0 failed\n",
      "\n",
      "[21/42] Cal Blueprint... ‚úì (About page)\n",
      "[22/42] CodeBase... ‚úì (About page)\n",
      "[23/42] Code the Change, Berkeley chapter... ‚úì (Homepage)\n",
      "[24/42] Cognitive Science Students Association (CSSA)... ‚úì (About page)\n",
      "[25/42] Computer Science Graduate Entrepreneurs (CSGE)... ‚úì (Homepage)\n",
      "[26/42] Computer Science Graduate Student Association (CSG... ‚ùå (timeout)\n",
      "[27/42] Computer Science KickStart (CSK)... ‚úì (Homepage)\n",
      "[28/42] Computer Science Mentors (CSM)... ‚úì (Homepage)\n",
      "[29/42] Computer Science Undergraduate Association (CSUA)... ‚úì (About page)\n",
      "[30/42] Data Science Society at Berkeley... ‚ö†Ô∏è  (minimal content - may be JS-heavy)\n",
      "\n",
      "  Progress: 26 success, 3 minimal, 1 failed\n",
      "\n",
      "[31/42] Game Design and Development Club... ‚úì (Homepage)\n",
      "[32/42] Generative AI at Berkeley... ‚ùå (timeout)\n",
      "[33/42] Hackers at Berkeley (HACK)... ‚úì (About page)\n",
      "[34/42] Hack the Bay (CalHacks)... ‚úì (About page)\n",
      "[35/42] Launchpad... ‚úì (About page)\n",
      "[36/42] Mobile Developers of Berkeley (MDB)... ‚úì (About page)\n",
      "[37/42] Open Computing Facility (OCF)... ‚úì (About page)\n",
      "[38/42] Philosophy of Computation at Berkeley (POCAB)... ‚úì (About page)\n",
      "[39/42] UC Berkeley Undergraduate Graphics Group (UCBUGG)... ‚úì (About page)\n",
      "[40/42] Undergraduate Group for Theoretical Computer Scien... ‚úì (Homepage)\n",
      "\n",
      "  Progress: 35 success, 3 minimal, 2 failed\n",
      "\n",
      "[41/42] Upsilon Pi Epsilon (UPE) Computer Science Honor So... ‚úì (Homepage)\n",
      "[42/42] Society of Women Engineers (SWE)... ‚úì (About page)\n",
      "\n",
      "‚úì Scraping complete!\n",
      "  Success: 37/42\n",
      "    - About pages: 22\n",
      "    - Homepages: 15\n",
      "  Minimal content: 3/42\n",
      "  Failed: 2/42\n",
      "\n",
      "\n",
      "üíæ Saving data...\n",
      "‚úì Saved CSV: eecs_organizations_complete.csv\n",
      "‚úì Saved JSON: eecs_organizations_complete.json\n",
      "\n",
      "======================================================================\n",
      "üìä DATA SUMMARY\n",
      "======================================================================\n",
      "Total organizations: 42\n",
      "\n",
      "Columns (8):\n",
      "  ‚Ä¢ name: 42/42 (100.0%)\n",
      "  ‚Ä¢ website: 42/42 (100.0%)\n",
      "  ‚Ä¢ eecs_summary: 41 with content\n",
      "  ‚Ä¢ email: 42/42 (100.0%)\n",
      "  ‚Ä¢ scraped_url: 42/42 (100.0%)\n",
      "  ‚Ä¢ has_about_page: 42/42 (100.0%)\n",
      "  ‚Ä¢ full_content: 32 with substantial content (>100 chars)\n",
      "  ‚Ä¢ status: 42/42 (100.0%)\n",
      "\n",
      "Sample organizations:\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìå The Association of Women in EE & CS (AWE)\n",
      "   Website: http://awe.berkeley.edu\n",
      "   EECS Summary: Formerly AUWICSEE. aweberkeley@gmail.com AWE is a supportive community at UC Berkeley that seeks to empower women pursuing Computer Science and/or Ele...\n",
      "   Email: aweberkeley@gmail.com\n",
      "   Scraped URL: https://awe.studentorg.berkeley.edu/home\n",
      "   Status: success\n",
      "   Website Content: AWE is a student-run organization at UC Berkeley that seeks to empower female and non-binary undergraduate students pursuing Computer Science and/or E...\n",
      "\n",
      "üìå Bias Busters\n",
      "   Website: https://biasbusters.berkeley.edu/\n",
      "   EECS Summary: biasbusters-admin@lists.eecs.berkeley.edu Bias Busters is an organization run by students, faculty, and staff to address implicit bias issues in the E...\n",
      "   Email: biasbusters-admin@lists.eecs.berkeley.edu\n",
      "   Scraped URL: https://biasbusters.berkeley.edu/about/\n",
      "   Status: success\n",
      "   Website Content: Home > AboutAbout Bias Busters is an organization run by graduate students, faculty, and staff to address implicit bias issues in our community. Our g...\n",
      "\n",
      "üìå Black Engineering and Science Student Association (BESSA)\n",
      "   Website: http://ucberkeleybessa.com\n",
      "   EECS Summary: ucberkeleybessa@gmail.com\n",
      "   Email: biasbusters-admin@lists.eecs.berkeley.edu\n",
      "   Scraped URL: http://ucberkeleybessa.com/about-us.html\n",
      "   Status: success\n",
      "   Website Content: ‚ÄãThe Black Engineering and Science Student Association, also known as BESSA, is the University of California, Berkeley chapter for our parent organiza...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "‚úÖ SCRAPING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üîí Closing browser...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üêª BERKELEY EECS ORGANIZATIONS SCRAPER (SELENIUM)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Handles JavaScript-heavy sites properly\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Get organizations from EECS page (no Selenium needed)\n",
    "    orgs = extract_organizations_from_eecs_page()\n",
    "    \n",
    "    if not orgs:\n",
    "        print(\"‚ùå Failed to fetch organizations list\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(orgs)} organizations.\")\n",
    "    print(\"\\n‚öôÔ∏è  Running in TEST MODE (first 5 organizations)\")\n",
    "    print(\"    (Change max_orgs=5 to max_orgs=None for full scrape)\\n\")\n",
    "    \n",
    "    max_orgs = None  # Change to None for full scrape\n",
    "    \n",
    "    if max_orgs:\n",
    "        print(f\"üß™ Scraping first {max_orgs} organizations\\n\")\n",
    "    else:\n",
    "        print(f\"üöÄ Scraping all {len(orgs)} organizations\")\n",
    "        print(f\"‚è±Ô∏è  Estimated time: {len(orgs) * 4 / 60:.0f}-{len(orgs) * 6 / 60:.0f} minutes\\n\")\n",
    "    \n",
    "    # Step 2: Setup Selenium\n",
    "    print(\"üåê Starting Chrome browser...\")\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Scrape each organization's website\n",
    "        enriched_orgs = scrape_all_organizations(driver, orgs, max_orgs=max_orgs)\n",
    "        \n",
    "        # Step 4: Save data\n",
    "        print(\"\\nüíæ Saving data...\")\n",
    "        prefix = 'eecs_orgs_test' if max_orgs else 'eecs_organizations_complete'\n",
    "        save_data(enriched_orgs, prefix=prefix)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚úÖ SCRAPING COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if max_orgs:\n",
    "            print(\"\\nüí° TIP: To scrape all organizations, change:\")\n",
    "            print(\"   max_orgs = 5\")\n",
    "            print(\"   to\")\n",
    "            print(\"   max_orgs = None\")\n",
    "        \n",
    "    finally:\n",
    "        print(\"\\nüîí Closing browser...\")\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
